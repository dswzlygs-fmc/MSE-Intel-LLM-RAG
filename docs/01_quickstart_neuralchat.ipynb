{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6ddfc4-ecd6-4f0a-9e32-5ff4ab014db7",
   "metadata": {},
   "source": [
    "# QuickStart: Intel® Extension For Transformers*: NeuralChat on 4th Generation Intel® Xeon® Scalable Processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a628ae6-ddd2-41d3-acf1-855b6f31aef5",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae386d8-6138-478d-ae4d-8572a561967c",
   "metadata": {},
   "source": [
    "Follow the README to install the necessary requirements to run this tutorial. In summary, you will need to install the following:\n",
    "-  Intel(R) Extension for Transformers* from source (to get latest updates)\n",
    "-  NeuralChat requirements\n",
    "-  Retrieval Plugin Requirements\n",
    "-  Audio Plugin (TTS and ASR) Requirements\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733fa69-5e68-453c-8f6f-401ee096f4a7",
   "metadata": {},
   "source": [
    "Check hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d39d47-087f-4e2a-b766-b3707ee342e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T13:46:00.664738Z",
     "iopub.status.busy": "2024-04-28T13:46:00.664389Z",
     "iopub.status.idle": "2024-04-28T13:46:04.168398Z",
     "shell.execute_reply": "2024-04-28T13:46:04.167544Z",
     "shell.execute_reply.started": "2024-04-28T13:46:00.664717Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/itrex/lib/python3.10/site-packages (5.9.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f88431c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T13:48:34.493879Z",
     "iopub.status.busy": "2024-04-28T13:48:34.493571Z",
     "iopub.status.idle": "2024-04-28T13:48:34.631490Z",
     "shell.execute_reply": "2024-04-28T13:48:34.630667Z",
     "shell.execute_reply.started": "2024-04-28T13:48:34.493860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu_count: 32\n",
      "cpu_count_logical: 64\n",
      "cpu_frequency: scpufreq(current=2699.9980000000005, min=0.0, max=0.0)\n",
      "virtual_memory: svmem(total=34359738368, available=33755369472, percent=1.8, used=600981504, free=1277616128, active=135979008, inactive=32937459712, buffers=0, cached=32481140736, shared=0, slab=0)\n",
      "disk_partitions: [sdiskpart(device='/dev/loop7', mountpoint='/tmp', fstype='ext4', opts='rw,relatime', maxfile=255, maxpath=4096), sdiskpart(device='/dev/loop7', mountpoint='/mnt/workspace', fstype='ext4', opts='rw,relatime', maxfile=255, maxpath=4096), sdiskpart(device='/dev/vda2', mountpoint='/etc/hosts', fstype='ext4', opts='rw,relatime', maxfile=255, maxpath=4096), sdiskpart(device='/dev/loop7', mountpoint='/etc/dsw-logs', fstype='ext4', opts='rw,relatime', maxfile=255, maxpath=4096), sdiskpart(device='/dev/vda2', mountpoint='/dev/termination-log', fstype='ext4', opts='rw,relatime', maxfile=255, maxpath=4096), sdiskpart(device='/dev/vda2', mountpoint='/etc/hostname', fstype='ext4', opts='rw,relatime', maxfile=255, maxpath=4096), sdiskpart(device='/dev/vda2', mountpoint='/etc/resolv.conf', fstype='ext4', opts='rw,relatime', maxfile=255, maxpath=4096), sdiskpart(device='/dev/loop7', mountpoint='/home/admin/workspace', fstype='ext4', opts='rw,relatime', maxfile=255, maxpath=4096), sdiskpart(device='/dev/loop7', mountpoint='/home/admin/logs', fstype='ext4', opts='rw,relatime', maxfile=255, maxpath=4096)]\n",
      "disk_usage: sdiskusage(total=106745946112, used=36580827136, free=64725946368, percent=36.1)\n",
      "network_io: snetio(bytes_sent=171149986, bytes_recv=15787749675, packets_sent=1424998, packets_recv=2644856, errin=0, errout=0, dropin=0, dropout=0)\n",
      "users: []\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def get_system_info():\n",
    "    system = {}\n",
    "    system['cpu_count'] = psutil.cpu_count(logical=False)  # Physical cores\n",
    "    system['cpu_count_logical'] = psutil.cpu_count()  # Total cores\n",
    "    system['cpu_frequency'] = psutil.cpu_freq()  # CPU frequency\n",
    "    system['virtual_memory'] = psutil.virtual_memory()  # Virtual memory\n",
    "    system['disk_partitions'] = psutil.disk_partitions()  # Disk partitions\n",
    "    system['disk_usage'] = psutil.disk_usage('/')  # Disk usage\n",
    "    system['network_io'] = psutil.net_io_counters()  # Network I/O statistics\n",
    "    system['users'] = psutil.users()  # Users logged on\n",
    "    return system\n",
    "\n",
    "system_info = get_system_info()\n",
    "for key, value in system_info.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a640c2",
   "metadata": {},
   "source": [
    "Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63fa0ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T13:48:34.488144Z",
     "iopub.status.busy": "2024-04-28T13:48:34.487494Z",
     "iopub.status.idle": "2024-04-28T13:48:34.492279Z",
     "shell.execute_reply": "2024-04-28T13:48:34.491648Z",
     "shell.execute_reply.started": "2024-04-28T13:48:34.488121Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6d1e2-61f1-4ee4-98c7-4f6202e7f2ea",
   "metadata": {},
   "source": [
    "## Building a Simple Chatbot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9791bad",
   "metadata": {},
   "source": [
    "Building a chatbot only requires the 3 lines of code below! By default, the model is Intel's Neural-Chat-7B-V3-1 model. Without any optimizations, the model runs in FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e630ebc8-6a1b-4c51-b3e1-8af098818671",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T13:55:52.101938Z",
     "iopub.status.busy": "2024-04-28T13:55:52.101561Z",
     "iopub.status.idle": "2024-04-28T13:55:52.126430Z",
     "shell.execute_reply": "2024-04-28T13:55:52.125326Z",
     "shell.execute_reply.started": "2024-04-28T13:55:52.101916Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WeightOnlyQuantConfig' from 'intel_extension_for_transformers.transformers' (/opt/conda/envs/itrex/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Build chatbot with INT4 weight-only quantization, computations in AMX INT8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mintel_extension_for_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_chat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_chatbot, PipelineConfig\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mintel_extension_for_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeightOnlyQuantConfig\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mintel_extension_for_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_chat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoadingModelConfig\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'WeightOnlyQuantConfig' from 'intel_extension_for_transformers.transformers' (/opt/conda/envs/itrex/lib/python3.10/site-packages/intel_extension_for_transformers/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Build chatbot with INT4 weight-only quantization, computations in AMX INT8\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "\n",
    "from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig\n",
    "from intel_extension_for_transformers.neural_chat.config import LoadingModelConfig\n",
    "\n",
    "\n",
    "\n",
    "config = PipelineConfig(model_name_or_path=\"./chatglm2-6b\",\n",
    "optimization_config=WeightOnlyQuantConfig(compute_dtype=\"int8\", \n",
    "weight_dtype=\"int4_fullrange\"),\n",
    "loading_config=LoadingModelConfig(use_neural_speed=False))\n",
    "chatbot = build_chatbot(config)\n",
    "# Perform inference/generate a response\n",
    "response = chatbot.predict(query=\"cnvrg.io 网站是由谁创建的？\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5555f5b-84d1-4154-9388-8aa17041fec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T13:48:34.632738Z",
     "iopub.status.busy": "2024-04-28T13:48:34.632420Z",
     "iopub.status.idle": "2024-04-28T13:48:42.671320Z",
     "shell.execute_reply": "2024-04-28T13:48:42.670477Z",
     "shell.execute_reply.started": "2024-04-28T13:48:34.632719Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "2024-04-28 21:48:42,525 - root - ERROR - Exception: Incorrect path_or_model_id: './chatglm2-6b\"'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\n",
      "2024-04-28 21:48:42,525 - intel_extension_for_transformers.neural_chat.utils.error_utils - ERROR - neuralchat error: Generic error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ./chatglm2-6b\"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Perform inference/generate a response\u001b[39;00m\n\u001b[1;32m      7\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchatbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about Intel Xeon Scalable Processors.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m end \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# Build chatbot\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "config = PipelineConfig(model_name_or_path='./chatglm2-6b\"')\n",
    "chatbot = build_chatbot(config)\n",
    "\n",
    "# Perform inference/generate a response\n",
    "start = time()\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "end = time()\n",
    "print(response)\n",
    "print(\"%.5f seconds\" %(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00b822-eef0-4b92-9a9a-9d7a8d148d6f",
   "metadata": {},
   "source": [
    "## Optimizing your Chatbot\n",
    "Enable mixed precision with bfloat16 (BF16). Using a lower precision data type will reduce memory usage and speed up runtime without sacrifice to accuracy, since BF16 has the same range as FP32, just less precision in terms of decimal places. Starting with the 4th Gen Intel® Xeon® Scalable Processors, there is an instruction set Advanced Matrix Extensions (AMX) which accelerates operators in BF16 and integer8 (INT8) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e832b1a-0f7a-4715-b245-85666729e206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build chatbot in BF16\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(model_name_or_path='Intel/neural-chat-7b-v3-1',\n",
    "                        optimization_config=MixedPrecisionConfig(dtype='bfloat16'))\n",
    "chatbot = build_chatbot(config)\n",
    "\n",
    "# Perform inference/generate a response\n",
    "start = time()\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "end = time()\n",
    "print(response)\n",
    "print(\"%.5f seconds\" %(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9ca77-149b-47cc-9a5b-a6f70877f09e",
   "metadata": {},
   "source": [
    "INT4 weight-only quantization can be used to further reduce memory and speed up performance without too much loss to accuracy. Note that the _compute_dtype_ is \"int8\" because AMX only supports down to INT8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804023f-5a79-47ce-9b4b-61ea9746df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build chatbot with INT4 weight-only quantization, computations in AMX INT8\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import RtnConfig\n",
    "from intel_extension_for_transformers.neural_chat.config import LoadingModelConfig\n",
    "config = PipelineConfig(model_name_or_path=\"Intel/neural-chat-7b-v3-1\",\n",
    "                        optimization_config=RtnConfig(bits=4, compute_dtype=\"int8\", weight_dtype=\"int4_fullrange\"), \n",
    "                        loading_config=LoadingModelConfig(use_neural_speed=False))\n",
    "chatbot = build_chatbot(config)\n",
    "\n",
    "# Perform inference/generate a response\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d7df2d",
   "metadata": {},
   "source": [
    "## Swapping out Models: Llama2 Example\n",
    "You can swap out the Neural-Chat-7B model with another transformer model from [HuggingFace](https://huggingface.co/models), including the most popular LLMs. Pass in the model card for the _model_name_or_path_ argument. For example, this is how you can build a chatbot using Llama2 in FP32 and BF16. *NOTE* You may need to log in to HuggingFace to get access to this model. To do so, use the command _huggingface-cli login_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad36497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: log in to HuggingFace to access Llama2\n",
    "export HUGGINGFACE_TOKEN=None #@TODO: enter in HF token here\n",
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build chatbot in BF16 using Llama2\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(model_name_or_path='meta-llama/Llama-2-7b-chat-hf',\n",
    "                        optimization_config=MixedPrecisionConfig(dtype='bfloat16'))\n",
    "chatbot = build_chatbot(config)\n",
    "\n",
    "# Perform inference/generate a response\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc089c9-623f-43ec-a470-31e5f8d4cac4",
   "metadata": {},
   "source": [
    "## Customizing your Chatbot\n",
    "### Plugin: Retrieval\n",
    "Without the retrieval plugin, the output of the chatbot gives the wrong answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b5c7d-15e0-4438-b451-9172159398fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "config = PipelineConfig(model_name_or_path='Intel/neural-chat-7b-v3-1')\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Who won Super Bowl 58 and what was the score?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65c5a6-1d33-43f6-8c61-92322b74e2cc",
   "metadata": {},
   "source": [
    "The retrieval plugin allows you to specify a file or a folder of files with information you want your chatbot to look up before outputting the final response. Here, _sample_workshop.txt_ contains the correct answer. For more information about the retrieval plugin and the file types supported, go to the [Retrieval README](https://github.com/intel/intel-extension-for-transformers/blob/main/intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval/README.md).\n",
    "\n",
    "You can specify a single file or a a folder. In this example, the files will be placed inside a folder _docs_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef691b-048a-4d5c-ada9-39acec7eeea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir docs\n",
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/assets/docs/sample_workshop.txt\n",
    "!mv sample_workshop.txt ./docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6474f-565b-4399-b250-a314447c2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./docs/sample_workshop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779e396-a963-4503-8a85-a0d897445c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build chatbot with retrieval\n",
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "plugins.retrieval.enable=True\n",
    "plugins.retrieval.args[\"input_path\"]=\"./docs/sample_workshop.txt\"\n",
    "config = PipelineConfig(model_name_or_path='Intel/neural-chat-7b-v3-1',\n",
    "                        plugins=plugins)\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Who won Super Bowl 58 and what was the score?\")\n",
    "print(response)\n",
    "\n",
    "plugins.retrieval.enable=False # disable retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da2eef-e383-4e54-b9c1-4079cf053a33",
   "metadata": {},
   "source": [
    "### Plugin: ASR & TTS\n",
    "The ASR and TTS plugin enables voice chat for a more interactive experience. Instead of passing in text and getting text responses, you can pass in audio files and get audio files in response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe3865-b23a-405d-8199-4a869f448ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/assets/audio/sample.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103340b5-1b57-486c-9fba-3060de63ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build chatbot with AST and TTS plugin\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "plugins.tts.enable = True\n",
    "plugins.tts.args[\"output_audio_path\"] = \"./response.wav\"\n",
    "plugins.asr.enable = True\n",
    "\n",
    "config = PipelineConfig(model_name_or_path='Intel/neural-chat-7b-v3-1',\n",
    "                        plugins=plugins)\n",
    "chatbot = build_chatbot(config)\n",
    "result = chatbot.predict(query=\"./sample.wav\")\n",
    "print(result)\n",
    "\n",
    "plugins.tts.enable = False # disable tts\n",
    "plugins.asr.enable = False # disable asr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7851393",
   "metadata": {},
   "source": [
    "Open the audio files using your own audio player to hear the query and response. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a0779-8c26-4c62-a514-64bf9c58896f",
   "metadata": {},
   "source": [
    "### [Optional]: Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8419deb-8a06-41ba-af86-875d1d1c0fc7",
   "metadata": {},
   "source": [
    "We use the [Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca) from Stanford University as the general domain dataset to fine-tune the model. This dataset is provided in the form of a JSON file, [alpaca_data.json](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json). In Alpaca, researchers have manually crafted 175 seed tasks to guide `text-davinci-003` in generating 52K instruction data for diverse tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5c30c-ecb4-4341-a586-4efdb3906bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a3a94-230a-497f-9ad9-2645b8f2d186",
   "metadata": {},
   "source": [
    "Finetune the model on Alpaca-format dataset to conduct text generation.\n",
    "\n",
    "We employ the [LoRA approach](https://arxiv.org/pdf/2106.09685.pdf) to finetune the LLM efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008618d-a82e-4249-813e-ae46b95c64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "model_args = ModelArguments(model_name_or_path=\"Intel/neural-chat-7b-v3-1\")\n",
    "data_args = DataArguments(train_file=\"alpaca_data.json\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./finetuned_model_path',\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    num_train_epochs=3,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True\n",
    ")\n",
    "finetune_args = FinetuningArguments()\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "            model_args=model_args,\n",
    "            data_args=data_args,\n",
    "            training_args=training_args,\n",
    "            finetune_args=finetune_args,\n",
    "        )\n",
    "finetune_model(finetune_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701abb4e-41fa-4aa4-81e0-2abc3f15f093",
   "metadata": {},
   "source": [
    "Load the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112b3b3-ef0d-4157-8c23-cf3ed5d59e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat.config import LoadingModelConfig\n",
    "\n",
    "config = PipelineConfig(model_name_or_path=\"Intel/neural-chat-7b-v3-1\",\n",
    "                      loading_config=LoadingModelConfig(peft_path=\"./finetuned_model_path\"))\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e46d1-3f94-423c-abcb-4a533777e26c",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed the NeuralChat quickstart. Now go build your own custom chatbots!\n",
    "Visit [notebooks directory](https://github.com/intel/intel-extension-for-transformers/blob/c30353fcb0e5ceab440a7508b5980ccebcac8750/intel_extension_for_transformers/neural_chat/docs/full_notebooks.md) to see more examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itrex",
   "language": "python",
   "name": "itrex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
