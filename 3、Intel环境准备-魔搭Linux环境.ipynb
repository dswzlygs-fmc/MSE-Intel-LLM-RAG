{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c31733-866e-4c84-b313-12075fab1f27",
   "metadata": {},
   "source": [
    "## 1、下载相关大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad35f58-4854-4528-a77b-3a61e33e5a43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T04:24:07.868045Z",
     "iopub.status.busy": "2024-05-14T04:24:07.867705Z",
     "iopub.status.idle": "2024-05-14T04:27:28.289088Z",
     "shell.execute_reply": "2024-05-14T04:27:28.288535Z",
     "shell.execute_reply.started": "2024-05-14T04:24:07.868017Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'chatglm3-6b'...\n",
      "remote: Enumerating objects: 140, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 140 (delta 8), reused 1 (delta 0), pack-reused 122\u001b[K\n",
      "接收对象中: 100% (140/140), 61.16 KiB | 10.19 MiB/s, 完成.\n",
      "处理 delta 中: 100% (60/60), 完成.\n",
      "过滤内容: 100% (15/15), 23.26 GiB | 119.49 MiB/s, 完成.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b331164-39ef-4113-8742-c00ba02ebf17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T04:27:33.624786Z",
     "iopub.status.busy": "2024-05-14T04:27:33.624456Z",
     "iopub.status.idle": "2024-05-14T04:27:49.634927Z",
     "shell.execute_reply": "2024-05-14T04:27:49.634340Z",
     "shell.execute_reply.started": "2024-05-14T04:27:33.624766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'bge-base-zh-v1.5'...\n",
      "remote: Enumerating objects: 30, done.\u001b[K\n",
      "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
      "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
      "remote: Total 30 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "接收对象中: 100% (30/30), 168.35 KiB | 1.08 MiB/s, 完成.\n",
      "处理 delta 中: 100% (5/5), 完成.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://www.modelscope.cn/AI-ModelScope/bge-base-zh-v1.5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d730c5f-0e5c-4b48-be6b-f30df900f58e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T05:47:45.004189Z",
     "iopub.status.busy": "2024-05-01T05:47:45.003860Z",
     "iopub.status.idle": "2024-05-01T05:49:28.816230Z",
     "shell.execute_reply": "2024-05-01T05:49:28.815688Z",
     "shell.execute_reply.started": "2024-05-01T05:47:45.004164Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "2024-05-01 13:47:55,701 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: ./bge-base-zh-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create retrieval plugin instance...\n",
      "plugin parameters:  {'embedding_model': './bge-base-zh-v1.5', 'input_path': './sample.jsonl'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 13:47:56,035 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
      "2024-05-01 13:47:56,038 - root - INFO - The parsing for the uploaded files is finished.\n",
      "2024-05-01 13:47:56,039 - root - INFO - The format of parsed documents is transferred.\n",
      "2024-05-01 13:47:56,051 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1585a6bbd832482592dccbfd125da90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 13:47:56,440 - root - INFO - The retriever is successfully built.\n",
      "2024-05-01 13:47:56,475 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting eos_token is not supported, use the default one.\n",
      "2024-05-01 13:47:56,476 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting pad_token is not supported, use the default one.\n",
      "2024-05-01 13:47:56,476 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting unk_token is not supported, use the default one.\n",
      "2024-05-01 13:47:56 [INFO] Applying Weight Only Quantization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ./chatglm3-6b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44d62d8ef9442c793618aa1803a1d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 13:48:27 [INFO] Start auto tuning.\n",
      "2024-05-01 13:48:27 [INFO] Quantize model without tuning!\n",
      "2024-05-01 13:48:27 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2024-05-01 13:48:27 [INFO] Adaptor has 5 recipes.\n",
      "2024-05-01 13:48:27 [INFO] 0 recipes specified by user.\n",
      "2024-05-01 13:48:27 [INFO] 3 recipes require future tuning.\n",
      "2024-05-01 13:48:27 [INFO] *** Initialize auto tuning\n",
      "2024-05-01 13:48:27 [INFO] {\n",
      "2024-05-01 13:48:27 [INFO]     'PostTrainingQuantConfig': {\n",
      "2024-05-01 13:48:27 [INFO]         'AccuracyCriterion': {\n",
      "2024-05-01 13:48:27 [INFO]             'criterion': 'relative',\n",
      "2024-05-01 13:48:27 [INFO]             'higher_is_better': True,\n",
      "2024-05-01 13:48:27 [INFO]             'tolerable_loss': 0.01,\n",
      "2024-05-01 13:48:27 [INFO]             'absolute': None,\n",
      "2024-05-01 13:48:27 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7f32a916dae0>>,\n",
      "2024-05-01 13:48:27 [INFO]             'relative': 0.01\n",
      "2024-05-01 13:48:27 [INFO]         },\n",
      "2024-05-01 13:48:27 [INFO]         'approach': 'post_training_weight_only',\n",
      "2024-05-01 13:48:27 [INFO]         'backend': 'default',\n",
      "2024-05-01 13:48:27 [INFO]         'calibration_sampling_size': [\n",
      "2024-05-01 13:48:27 [INFO]             100\n",
      "2024-05-01 13:48:27 [INFO]         ],\n",
      "2024-05-01 13:48:27 [INFO]         'device': 'cpu',\n",
      "2024-05-01 13:48:27 [INFO]         'diagnosis': False,\n",
      "2024-05-01 13:48:27 [INFO]         'domain': 'auto',\n",
      "2024-05-01 13:48:27 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2024-05-01 13:48:27 [INFO]         'excluded_precisions': [\n",
      "2024-05-01 13:48:27 [INFO]         ],\n",
      "2024-05-01 13:48:27 [INFO]         'framework': 'pytorch_fx',\n",
      "2024-05-01 13:48:27 [INFO]         'inputs': [\n",
      "2024-05-01 13:48:27 [INFO]         ],\n",
      "2024-05-01 13:48:27 [INFO]         'model_name': '',\n",
      "2024-05-01 13:48:27 [INFO]         'ni_workload_name': 'quantization',\n",
      "2024-05-01 13:48:27 [INFO]         'op_name_dict': {\n",
      "2024-05-01 13:48:27 [INFO]             '.*lm_head': {\n",
      "2024-05-01 13:48:27 [INFO]                 'weight': {\n",
      "2024-05-01 13:48:27 [INFO]                     'dtype': [\n",
      "2024-05-01 13:48:27 [INFO]                         'fp32'\n",
      "2024-05-01 13:48:27 [INFO]                     ]\n",
      "2024-05-01 13:48:27 [INFO]                 }\n",
      "2024-05-01 13:48:27 [INFO]             },\n",
      "2024-05-01 13:48:27 [INFO]             '.*output_layer': {\n",
      "2024-05-01 13:48:27 [INFO]                 'weight': {\n",
      "2024-05-01 13:48:27 [INFO]                     'dtype': [\n",
      "2024-05-01 13:48:27 [INFO]                         'fp32'\n",
      "2024-05-01 13:48:27 [INFO]                     ]\n",
      "2024-05-01 13:48:27 [INFO]                 }\n",
      "2024-05-01 13:48:27 [INFO]             },\n",
      "2024-05-01 13:48:27 [INFO]             '.*embed_out': {\n",
      "2024-05-01 13:48:27 [INFO]                 'weight': {\n",
      "2024-05-01 13:48:27 [INFO]                     'dtype': [\n",
      "2024-05-01 13:48:27 [INFO]                         'fp32'\n",
      "2024-05-01 13:48:27 [INFO]                     ]\n",
      "2024-05-01 13:48:27 [INFO]                 }\n",
      "2024-05-01 13:48:27 [INFO]             }\n",
      "2024-05-01 13:48:27 [INFO]         },\n",
      "2024-05-01 13:48:27 [INFO]         'op_type_dict': {\n",
      "2024-05-01 13:48:27 [INFO]             '.*': {\n",
      "2024-05-01 13:48:27 [INFO]                 'weight': {\n",
      "2024-05-01 13:48:27 [INFO]                     'bits': [\n",
      "2024-05-01 13:48:27 [INFO]                         4\n",
      "2024-05-01 13:48:27 [INFO]                     ],\n",
      "2024-05-01 13:48:27 [INFO]                     'dtype': [\n",
      "2024-05-01 13:48:27 [INFO]                         'int4'\n",
      "2024-05-01 13:48:27 [INFO]                     ],\n",
      "2024-05-01 13:48:27 [INFO]                     'group_size': [\n",
      "2024-05-01 13:48:27 [INFO]                         32\n",
      "2024-05-01 13:48:27 [INFO]                     ],\n",
      "2024-05-01 13:48:27 [INFO]                     'scheme': [\n",
      "2024-05-01 13:48:27 [INFO]                         'sym'\n",
      "2024-05-01 13:48:27 [INFO]                     ],\n",
      "2024-05-01 13:48:27 [INFO]                     'algorithm': [\n",
      "2024-05-01 13:48:27 [INFO]                         'RTN'\n",
      "2024-05-01 13:48:27 [INFO]                     ]\n",
      "2024-05-01 13:48:27 [INFO]                 }\n",
      "2024-05-01 13:48:27 [INFO]             }\n",
      "2024-05-01 13:48:27 [INFO]         },\n",
      "2024-05-01 13:48:27 [INFO]         'outputs': [\n",
      "2024-05-01 13:48:27 [INFO]         ],\n",
      "2024-05-01 13:48:27 [INFO]         'quant_format': 'default',\n",
      "2024-05-01 13:48:27 [INFO]         'quant_level': 'auto',\n",
      "2024-05-01 13:48:27 [INFO]         'recipes': {\n",
      "2024-05-01 13:48:27 [INFO]             'smooth_quant': False,\n",
      "2024-05-01 13:48:27 [INFO]             'smooth_quant_args': {\n",
      "2024-05-01 13:48:27 [INFO]             },\n",
      "2024-05-01 13:48:27 [INFO]             'layer_wise_quant': False,\n",
      "2024-05-01 13:48:27 [INFO]             'layer_wise_quant_args': {\n",
      "2024-05-01 13:48:27 [INFO]             },\n",
      "2024-05-01 13:48:27 [INFO]             'fast_bias_correction': False,\n",
      "2024-05-01 13:48:27 [INFO]             'weight_correction': False,\n",
      "2024-05-01 13:48:27 [INFO]             'gemm_to_matmul': True,\n",
      "2024-05-01 13:48:27 [INFO]             'graph_optimization_level': None,\n",
      "2024-05-01 13:48:27 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2024-05-01 13:48:27 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2024-05-01 13:48:27 [INFO]             'pre_post_process_quantization': True,\n",
      "2024-05-01 13:48:27 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2024-05-01 13:48:27 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2024-05-01 13:48:27 [INFO]             ],\n",
      "2024-05-01 13:48:27 [INFO]             'dedicated_qdq_pair': False,\n",
      "2024-05-01 13:48:27 [INFO]             'rtn_args': {\n",
      "2024-05-01 13:48:27 [INFO]                 'enable_full_range': True,\n",
      "2024-05-01 13:48:27 [INFO]                 'enable_mse_search': False\n",
      "2024-05-01 13:48:27 [INFO]             },\n",
      "2024-05-01 13:48:27 [INFO]             'awq_args': {\n",
      "2024-05-01 13:48:27 [INFO]             },\n",
      "2024-05-01 13:48:27 [INFO]             'gptq_args': {\n",
      "2024-05-01 13:48:27 [INFO]             },\n",
      "2024-05-01 13:48:27 [INFO]             'teq_args': {\n",
      "2024-05-01 13:48:27 [INFO]             },\n",
      "2024-05-01 13:48:27 [INFO]             'autoround_args': {\n",
      "2024-05-01 13:48:27 [INFO]             }\n",
      "2024-05-01 13:48:27 [INFO]         },\n",
      "2024-05-01 13:48:27 [INFO]         'reduce_range': None,\n",
      "2024-05-01 13:48:27 [INFO]         'TuningCriterion': {\n",
      "2024-05-01 13:48:27 [INFO]             'max_trials': 100,\n",
      "2024-05-01 13:48:27 [INFO]             'objective': [\n",
      "2024-05-01 13:48:27 [INFO]                 'performance'\n",
      "2024-05-01 13:48:27 [INFO]             ],\n",
      "2024-05-01 13:48:27 [INFO]             'strategy': 'basic',\n",
      "2024-05-01 13:48:27 [INFO]             'strategy_kwargs': None,\n",
      "2024-05-01 13:48:27 [INFO]             'timeout': 0\n",
      "2024-05-01 13:48:27 [INFO]         },\n",
      "2024-05-01 13:48:27 [INFO]         'use_bf16': True\n",
      "2024-05-01 13:48:27 [INFO]     }\n",
      "2024-05-01 13:48:27 [INFO] }\n",
      "2024-05-01 13:48:27 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2024-05-01 13:48:27 [INFO] Pass query framework capability elapsed time: 3.58 ms\n",
      "2024-05-01 13:48:27 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2024-05-01 13:48:27 [INFO] Quantize the model with default config.\n",
      "2024-05-01 13:48:27 [INFO] All algorithms to do: {'RTN'}\n",
      "2024-05-01 13:48:27 [INFO] quantizing with the round-to-nearest algorithm\n",
      "2024-05-01 13:48:40 [INFO] |******Mixed Precision Statistics******|\n",
      "2024-05-01 13:48:40 [INFO] +---------+-------+-----------+--------+\n",
      "2024-05-01 13:48:40 [INFO] | Op Type | Total |  A32W4G32 |  FP32  |\n",
      "2024-05-01 13:48:40 [INFO] +---------+-------+-----------+--------+\n",
      "2024-05-01 13:48:40 [INFO] |  Linear |  113  |    112    |   1    |\n",
      "2024-05-01 13:48:40 [INFO] +---------+-------+-----------+--------+\n",
      "2024-05-01 13:48:40 [INFO] Pass quantize model elapsed time: 12373.52 ms\n",
      "2024-05-01 13:48:40 [INFO] Save tuning history to /mnt/workspace/Intel_RagNeo4j/nc_workspace/2024-05-01_13-47-48/./history.snapshot.\n",
      "2024-05-01 13:48:40 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2024-05-01 13:48:40 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2024-05-01 13:48:40 [INFO] Save deploy yaml to /mnt/workspace/Intel_RagNeo4j/nc_workspace/2024-05-01_13-47-48/deploy.yaml\n",
      "2024-05-01 13:49:28 [INFO] WeightOnlyQuant done.\n",
      "2024-05-01 13:49:28,813 - root - INFO - Optimized Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "from intel_extension_for_transformers.transformers import RtnConfig\n",
    "plugins.retrieval.enable=True\n",
    "plugins.retrieval.args['embedding_model'] = \"./bge-base-zh-v1.5\"\n",
    "plugins.retrieval.args[\"input_path\"]=\"./sample.jsonl\"\n",
    "config = PipelineConfig(model_name_or_path='./chatglm3-6b',\n",
    " plugins=plugins,\n",
    " optimization_config=RtnConfig(compute_dtype=\"int8\",\n",
    "weight_dtype=\"int4_fullrange\"))\n",
    "chatbot = build_chatbot(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248d3c01-5d69-4796-a418-6572f34c9d10",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-01T05:50:32.385304Z",
     "iopub.status.busy": "2024-05-01T05:50:32.384971Z",
     "iopub.status.idle": "2024-05-01T05:50:51.605863Z",
     "shell.execute_reply": "2024-05-01T05:50:51.605127Z",
     "shell.execute_reply.started": "2024-05-01T05:50:32.385284Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/torch/amp/autocast_mode.py:267: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnvrg.io 网站是由一个名为CNVRG的神秘组织创建的。关于这个组织的具体信息很难获取，因为它们似乎非常低调。然而，我们可以看到网站上留下的联系信息，其中提到了一个电子邮件地址（[contact@cnvrg.io](mailto:contact@cnvrg.io)），但目前没有其他详细信息来证实这个组织的身份。\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "plugins.retrieval.enable=False # disable retrieval\n",
    "response = chatbot.predict(query=\"cnvrg.io 网站是由谁创建的？\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92cbb91-200a-4f12-898b-ff38466d1ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T05:55:08.451846Z",
     "iopub.status.busy": "2024-05-01T05:55:08.451449Z",
     "iopub.status.idle": "2024-05-01T05:55:32.384372Z",
     "shell.execute_reply": "2024-05-01T05:55:32.383701Z",
     "shell.execute_reply.started": "2024-05-01T05:55:08.451817Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e8aea7ca964dd584626f9dcbf8469e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 13:55:19,703 - root - INFO - Chat with QA Agent.\n",
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/torch/amp/autocast_mode.py:267: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抱歉，我无法回答您的问题。根据我的搜索结果，没有找到有关 \"cnvrg.io\" 网站的创建者信息。请提供更多详细信息以便我能更好地帮助您。\n"
     ]
    }
   ],
   "source": [
    "plugins.retrieval.enable=True # enable retrieval\n",
    "response = chatbot.predict(query=\"cnvrg.io 网站是由谁创建的？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ab44f-b387-4db6-91a5-cecd267c42f3",
   "metadata": {},
   "source": [
    "## 2、首先使用原始大模型，不进行量化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c46afb-6ac4-499b-b2e4-9378d3bad5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_org = PipelineConfig(model_name_or_path='./chatglm3-6b')\n",
    "chatbot_llm = build_chatbot(config_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab202a-b051-4864-b9e5-58c8b9876332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用未经量化的原始模型\n",
    "response = chatbot_llm.predict(query=\"你是谁？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef6eda-10b4-41bc-abb3-ce1ab4353020",
   "metadata": {},
   "source": [
    "## 3、封装Intel大模型对象，使其适应LangChain调用所需"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed7f7b2a-0892-49dd-83b9-98a9ba01c321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T05:56:09.830783Z",
     "iopub.status.busy": "2024-05-01T05:56:09.830410Z",
     "iopub.status.idle": "2024-05-01T05:56:13.807087Z",
     "shell.execute_reply": "2024-05-01T05:56:13.806411Z",
     "shell.execute_reply.started": "2024-05-01T05:56:09.830761Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: langchain in /opt/conda/envs/itrex/lib/python3.10/site-packages (0.1.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (0.0.27)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (0.1.40)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (0.1.40)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (1.10.13)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/itrex/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10c2f612-cc3f-4a6e-958c-d3a9fc2e057a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-01T06:54:29.114546Z",
     "iopub.status.busy": "2024-05-01T06:54:29.114221Z",
     "iopub.status.idle": "2024-05-01T06:54:29.128076Z",
     "shell.execute_reply": "2024-05-01T06:54:29.127309Z",
     "shell.execute_reply.started": "2024-05-01T06:54:29.114526Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Union\n",
    "import logging\n",
    "import json\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_community.llms.utils import enforce_stop_tokens\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.pydantic_v1 import Field\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from intel_extension_for_transformers.neural_chat.models.chatglm_model import ChatGlmModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def _convert_message_to_dict(message: BaseMessage) -> dict:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        message_dict = {\"role\": \"user\", \"content\": message.content}\n",
    "    elif isinstance(message, AIMessage):\n",
    "        message_dict = {\"role\": \"assistant\", \"content\": message.content}\n",
    "    elif isinstance(message, SystemMessage):\n",
    "        message_dict = {\"role\": \"system\", \"content\": message.content}\n",
    "    elif isinstance(message, FunctionMessage):\n",
    "        message_dict = {\"role\": \"function\", \"content\": message.content}\n",
    "    else:\n",
    "        raise ValueError(f\"Got unknown type {message}\")\n",
    "    return message_dict\n",
    "\n",
    "class ChatGLM3(LLM):\n",
    "    model_name: str = Field(default=\"chatglm3-6b\", alias=\"model\")\n",
    "    model_kwargs: Optional[dict] = None\n",
    "    \"\"\"Keyword arguments to pass to the model.\"\"\"\n",
    "    temperature: float = 0.1\n",
    "    \"\"\"LLM model temperature from 0 to 10.\"\"\"\n",
    "    top_p: float = 0.7\n",
    "    \"\"\"Top P for nucleus sampling from 0 to 1\"\"\"\n",
    "    prefix_messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    \"\"\"Series of messages for Chat input.\"\"\"\n",
    "    local_llm: ChatGlmModel = None\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Local_Intel_chatglm3-6b\"\n",
    "    \n",
    "    @property\n",
    "    def _invocation_params(self) -> dict:\n",
    "        \"\"\"Get the parameters used to invoke the model.\"\"\"\n",
    "        params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"top_p\": self.top_p,\n",
    "        }\n",
    "        return {**params, **(self.model_kwargs or {})}\n",
    "    \n",
    "    def _get_payload(self, prompt: str) -> dict:\n",
    "        params = self._invocation_params\n",
    "        messages = self.prefix_messages + [HumanMessage(content=prompt)]\n",
    "        params.update(\n",
    "            {\n",
    "                \"messages\": [_convert_message_to_dict(m) for m in messages],\n",
    "            }\n",
    "        )\n",
    "        return params\n",
    "        \n",
    "    def setClient(self, client: ChatGlmModel) -> ChatGlmModel:\n",
    "        if client:\n",
    "            self.local_llm = client\n",
    "        return self.local_llm \n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call out to a ChatGLM3 LLM inference endpoint.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to pass into the model.\n",
    "            stop: Optional list of stop words to use when generating.\n",
    "\n",
    "        Returns:\n",
    "            The string generated by the model.\n",
    "\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "\n",
    "                response = chatglm_llm(\"Who are you?\")\n",
    "        \"\"\"\n",
    "        import httpx\n",
    "\n",
    "        payload = self._get_payload(prompt)\n",
    "        logger.debug(f\"ChatGLM3 payload: {payload}\")\n",
    "\n",
    "        try:\n",
    "            # response = self.client.post(\n",
    "            #     self.endpoint_url, headers=HEADERS, json=payload\n",
    "            # )\n",
    "            response = self.local_llm.predict(query=prompt)\n",
    "        except httpx.NetworkError as e:\n",
    "            raise ValueError(f\"Error raised by inference endpoint: {e}\")\n",
    "\n",
    "        logger.debug(f\"ChatGLM3 response: {response}\")\n",
    "        return response\n",
    "        # if response.status_code != 200:\n",
    "        #     raise ValueError(f\"Failed with response: {response}\")\n",
    "\n",
    "        try:\n",
    "            parsed_response = response.json()\n",
    "\n",
    "            if isinstance(parsed_response, dict):\n",
    "                content_keys = \"choices\"\n",
    "                if content_keys in parsed_response:\n",
    "                    choices = parsed_response[content_keys]\n",
    "                    if len(choices):\n",
    "                        text = choices[0][\"message\"][\"content\"]\n",
    "                else:\n",
    "                    raise ValueError(f\"No content in response : {parsed_response}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected response type: {parsed_response}\")\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(\n",
    "                f\"Error raised during decoding response from inference endpoint: {e}.\"\n",
    "                f\"\\nResponse: {response.text}\"\n",
    "            )\n",
    "\n",
    "        if stop is not None:\n",
    "            text = enforce_stop_tokens(text, stop)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "860c72d2-5419-49b3-83a6-5355bc99f8d1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-01T06:54:32.670044Z",
     "iopub.status.busy": "2024-05-01T06:54:32.669358Z",
     "iopub.status.idle": "2024-05-01T06:54:55.589700Z",
     "shell.execute_reply": "2024-05-01T06:54:55.589111Z",
     "shell.execute_reply.started": "2024-05-01T06:54:32.670018Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/torch/amp/autocast_mode.py:267: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f28d172c804ae9b0418c1fe87ae857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 14:54:41,572 - root - INFO - Chat with QA Agent.\n",
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/torch/amp/autocast_mode.py:267: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我是助手，我的目标是帮助您回答问题和提供信息。我参考了本地知识库的搜索结果，但不会提供我认为不相关或不准确的信息。如果您不知道答案，我会耐心等待，或者尝试用其他方式来帮助您。'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customLLM = ChatGLM3()\n",
    "customLLM.setClient(client=chatbot)\n",
    "customLLM.invoke(\"你是谁？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35da68-c24f-403c-bd5a-f09bfad450d3",
   "metadata": {},
   "source": [
    "如上图所示，通过继承Langchain中的类，可以调用本地Chatglm3模型。\n",
    "\n",
    "至此完成Intel基础环境准备与测试"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itrex",
   "language": "python",
   "name": "itrex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
