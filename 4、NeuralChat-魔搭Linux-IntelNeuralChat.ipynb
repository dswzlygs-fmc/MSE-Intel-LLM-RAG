{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备并测试NeuralChat环境\n",
    "\n",
    "注：本测试均在本地Windows环境下运行\n",
    "\n",
    "参考资料：\n",
    "https://github.com/intel/intel-extension-for-transformers/blob/main/intel_extension_for_transformers/neural_chat/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用本地的chatglm3-6b 模型，并进行量化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-14T04:28:34.928558Z",
     "iopub.status.busy": "2024-05-14T04:28:34.928234Z",
     "iopub.status.idle": "2024-05-14T04:29:16.112554Z",
     "shell.execute_reply": "2024-05-14T04:29:16.112097Z",
     "shell.execute_reply.started": "2024-05-14T04:28:34.928539Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "2024-05-14 12:28:43,321 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting eos_token is not supported, use the default one.\n",
      "2024-05-14 12:28:43,321 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting pad_token is not supported, use the default one.\n",
      "2024-05-14 12:28:43,322 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ./chatglm3-6b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560e51937e73408c85a3f8304765f2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/intel_extension_for_pytorch/frontend.py:462: UserWarning: Conv BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/intel_extension_for_pytorch/frontend.py:469: UserWarning: Linear BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\n",
      "2024-05-14 12:29:16,110 - root - INFO - Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "from intel_extension_for_transformers.transformers import RtnConfig\n",
    "config_local = PipelineConfig(model_name_or_path='./chatglm3-6b')\n",
    "chatbot = build_chatbot(config_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T05:17:46.601059Z",
     "iopub.status.busy": "2024-05-14T05:17:46.600694Z",
     "iopub.status.idle": "2024-05-14T05:17:53.177981Z",
     "shell.execute_reply": "2024-05-14T05:17:53.177503Z",
     "shell.execute_reply.started": "2024-05-14T05:17:46.601039Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "服务调用花费了 6.573718547821045 秒\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.time()\n",
    "question = \"\"\"你好，我想咨询一些医疗方面的信息，你能帮助我么？ \"\"\"\n",
    "response = chatbot.predict(question)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"服务调用花费了 {elapsed_time} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T05:19:06.706010Z",
     "iopub.status.busy": "2024-05-14T05:19:06.705659Z",
     "iopub.status.idle": "2024-05-14T05:19:28.376236Z",
     "shell.execute_reply": "2024-05-14T05:19:28.375769Z",
     "shell.execute_reply.started": "2024-05-14T05:19:06.705990Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "服务调用花费了 21.66703701019287 秒\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "question = \"\"\"我被开水烫伤了左手，请问应该如何处理？ \"\"\"\n",
    "response = chatbot.predict(question)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"服务调用花费了 {elapsed_time} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-05-14T05:11:56.743928Z",
     "iopub.status.busy": "2024-05-14T05:11:56.743577Z",
     "iopub.status.idle": "2024-05-14T05:12:12.632490Z",
     "shell.execute_reply": "2024-05-14T05:12:12.631903Z",
     "shell.execute_reply.started": "2024-05-14T05:11:56.743900Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好，我已经找到了名为\"Christina Williams\"的患者的ID和血型信息。她的ID是123456，血型是O型。\n"
     ]
    }
   ],
   "source": [
    "#response = chatbot.predict(\"你是谁？\")\n",
    "\n",
    "#回答是瞎扯的\n",
    "response = chatbot.predict( \"\"\"Please look for the ID and blood type of the Patient named \"Christina Williams\", I think the field name of blood type will be like patient blood type. \"\"\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 继承Langchain的LLM类，为后续调用作准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T04:30:10.349271Z",
     "iopub.status.busy": "2024-05-14T04:30:10.348759Z",
     "iopub.status.idle": "2024-05-14T04:30:10.363509Z",
     "shell.execute_reply": "2024-05-14T04:30:10.363022Z",
     "shell.execute_reply.started": "2024-05-14T04:30:10.349250Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Union\n",
    "import logging\n",
    "import json\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_community.llms.utils import enforce_stop_tokens\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.pydantic_v1 import Field\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from intel_extension_for_transformers.neural_chat.models.chatglm_model import ChatGlmModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def _convert_message_to_dict(message: BaseMessage) -> dict:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        message_dict = {\"role\": \"user\", \"content\": message.content}\n",
    "    elif isinstance(message, AIMessage):\n",
    "        message_dict = {\"role\": \"assistant\", \"content\": message.content}\n",
    "    elif isinstance(message, SystemMessage):\n",
    "        message_dict = {\"role\": \"system\", \"content\": message.content}\n",
    "    elif isinstance(message, FunctionMessage):\n",
    "        message_dict = {\"role\": \"function\", \"content\": message.content}\n",
    "    else:\n",
    "        raise ValueError(f\"Got unknown type {message}\")\n",
    "    return message_dict\n",
    "\n",
    "class ChatGLM3(LLM):\n",
    "    model_name: str = Field(default=\"chatglm3-6b\", alias=\"model\")\n",
    "    model_kwargs: Optional[dict] = None\n",
    "    \"\"\"Keyword arguments to pass to the model.\"\"\"\n",
    "    temperature: float = 0.1\n",
    "    \"\"\"LLM model temperature from 0 to 10.\"\"\"\n",
    "    top_p: float = 0.7\n",
    "    \"\"\"Top P for nucleus sampling from 0 to 1\"\"\"\n",
    "    prefix_messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    \"\"\"Series of messages for Chat input.\"\"\"\n",
    "    local_llm: ChatGlmModel = None\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Local_Intel_chatglm3-6b\"\n",
    "    \n",
    "    @property\n",
    "    def _invocation_params(self) -> dict:\n",
    "        \"\"\"Get the parameters used to invoke the model.\"\"\"\n",
    "        params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"top_p\": self.top_p,\n",
    "        }\n",
    "        return {**params, **(self.model_kwargs or {})}\n",
    "    \n",
    "    def _get_payload(self, prompt: str) -> dict:\n",
    "        params = self._invocation_params\n",
    "        messages = self.prefix_messages + [HumanMessage(content=prompt)]\n",
    "        params.update(\n",
    "            {\n",
    "                \"messages\": [_convert_message_to_dict(m) for m in messages],\n",
    "            }\n",
    "        )\n",
    "        return params\n",
    "        \n",
    "    def setClient(self, client: ChatGlmModel) -> ChatGlmModel:\n",
    "        if client:\n",
    "            self.local_llm = client\n",
    "        return self.local_llm \n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call out to a ChatGLM3 LLM inference endpoint.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to pass into the model.\n",
    "            stop: Optional list of stop words to use when generating.\n",
    "\n",
    "        Returns:\n",
    "            The string generated by the model.\n",
    "\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "\n",
    "                response = chatglm_llm(\"Who are you?\")\n",
    "        \"\"\"\n",
    "        import httpx\n",
    "\n",
    "        payload = self._get_payload(prompt)\n",
    "        logger.debug(f\"ChatGLM3 payload: {payload}\")\n",
    "\n",
    "        try:\n",
    "            # response = self.client.post(\n",
    "            #     self.endpoint_url, headers=HEADERS, json=payload\n",
    "            # )\n",
    "            response = self.local_llm.predict(query=prompt)\n",
    "        except httpx.NetworkError as e:\n",
    "            raise ValueError(f\"Error raised by inference endpoint: {e}\")\n",
    "\n",
    "        logger.debug(f\"ChatGLM3 response: {response}\")\n",
    "        return response\n",
    "        # if response.status_code != 200:\n",
    "        #     raise ValueError(f\"Failed with response: {response}\")\n",
    "\n",
    "        try:\n",
    "            parsed_response = response.json()\n",
    "\n",
    "            if isinstance(parsed_response, dict):\n",
    "                content_keys = \"choices\"\n",
    "                if content_keys in parsed_response:\n",
    "                    choices = parsed_response[content_keys]\n",
    "                    if len(choices):\n",
    "                        text = choices[0][\"message\"][\"content\"]\n",
    "                else:\n",
    "                    raise ValueError(f\"No content in response : {parsed_response}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected response type: {parsed_response}\")\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(\n",
    "                f\"Error raised during decoding response from inference endpoint: {e}.\"\n",
    "                f\"\\nResponse: {response.text}\"\n",
    "            )\n",
    "\n",
    "        if stop is not None:\n",
    "            text = enforce_stop_tokens(text, stop)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-14T05:04:37.485350Z",
     "iopub.status.busy": "2024-05-14T05:04:37.485009Z",
     "iopub.status.idle": "2024-05-14T05:04:44.047332Z",
     "shell.execute_reply": "2024-05-14T05:04:44.046772Z",
     "shell.execute_reply.started": "2024-05-14T05:04:37.485330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "服务调用花费了 6.558316469192505 秒\n"
     ]
    }
   ],
   "source": [
    "customLLM = ChatGLM3()\n",
    "customLLM.setClient(client=chatbot)\n",
    "\n",
    "start_time = time.time()\n",
    "question = \"\"\"你好，我想咨询一些医疗方面的信息，你能帮助我么？ \"\"\"\n",
    "customLLM.invoke(question)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"服务调用花费了 {elapsed_time} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T05:09:51.933621Z",
     "iopub.status.busy": "2024-05-14T05:09:51.933300Z",
     "iopub.status.idle": "2024-05-14T05:10:19.922832Z",
     "shell.execute_reply": "2024-05-14T05:10:19.922184Z",
     "shell.execute_reply.started": "2024-05-14T05:09:51.933600Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "服务调用花费了 27.98581600189209 秒\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "question = \"\"\"我被开水烫伤了左手，请问应该如何处理？ \"\"\"\n",
    "customLLM.invoke(question)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"服务调用花费了 {elapsed_time} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T05:09:10.730374Z",
     "iopub.status.busy": "2024-05-14T05:09:10.730049Z",
     "iopub.status.idle": "2024-05-14T05:09:27.086155Z",
     "shell.execute_reply": "2024-05-14T05:09:27.085681Z",
     "shell.execute_reply.started": "2024-05-14T05:09:10.730355Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "服务调用花费了 16.352635383605957 秒\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "question = \"\"\"Please look for the ID and blood type of the Patient named \"Christina Williams\", I think the field name of blood type will be like patient blood type. \"\"\"\n",
    "customLLM.invoke(question)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"服务调用花费了 {elapsed_time} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用相同的Neo4j组件，完成相同操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、准备SSH隧道，允许当前环境访问本地Neo4j数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、定义工具包，包含Cypher查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T04:31:46.286943Z",
     "iopub.status.busy": "2024-05-14T04:31:46.286601Z",
     "iopub.status.idle": "2024-05-14T04:31:50.188115Z",
     "shell.execute_reply": "2024-05-14T04:31:50.187238Z",
     "shell.execute_reply.started": "2024-05-14T04:31:46.286922Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: neo4j in /opt/conda/lib/python3.10/site-packages (5.20.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from neo4j) (2023.4)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-14T04:37:57.405664Z",
     "iopub.status.busy": "2024-05-14T04:37:57.405335Z",
     "iopub.status.idle": "2024-05-14T04:37:57.630469Z",
     "shell.execute_reply": "2024-05-14T04:37:57.629988Z",
     "shell.execute_reply.started": "2024-05-14T04:37:57.405643Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1994/10/6\",\n",
      "            \"patient_sex\": \"Female\",\n",
      "            \"blood_type\": \"O+\",\n",
      "            \"name\": \"Tiffany Ramirez\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"0\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1973/3/31\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"A-\",\n",
      "            \"name\": \"Ruben Burns\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"1\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1932/5/10\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"O-\",\n",
      "            \"name\": \"Chad Byrd\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"2\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1944/10/4\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"AB+\",\n",
      "            \"name\": \"Antonio Frederick\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"3\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1989/1/26\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"AB+\",\n",
      "            \"name\": \"Mrs. Brandy Flowers\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"4\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1962/10/4\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"A+\",\n",
      "            \"name\": \"Patrick Parker\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"5\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1960/11/23\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"B-\",\n",
      "            \"name\": \"Charles Horton\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"6\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1990/7/23\",\n",
      "            \"patient_sex\": \"Female\",\n",
      "            \"blood_type\": \"B-\",\n",
      "            \"name\": \"Patty Norman\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"7\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1947/2/26\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"O-\",\n",
      "            \"name\": \"Ryan Hayes\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"8\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1976/2/19\",\n",
      "            \"patient_sex\": \"Female\",\n",
      "            \"blood_type\": \"AB-\",\n",
      "            \"name\": \"Sharon Perez\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"9\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1942/2/13\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"AB+\",\n",
      "            \"name\": \"Amy Roberts\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"10\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1990/2/17\",\n",
      "            \"patient_sex\": \"Female\",\n",
      "            \"blood_type\": \"A+\",\n",
      "            \"name\": \"Mrs. Caroline Farrell\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"11\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1969/6/28\",\n",
      "            \"patient_sex\": \"Female\",\n",
      "            \"blood_type\": \"O-\",\n",
      "            \"name\": \"Christina Williams\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"12\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1961/2/10\",\n",
      "            \"patient_sex\": \"Female\",\n",
      "            \"blood_type\": \"B-\",\n",
      "            \"name\": \"William Page\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"13\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1973/11/14\",\n",
      "            \"patient_sex\": \"Female\",\n",
      "            \"blood_type\": \"AB+\",\n",
      "            \"name\": \"Michael Bradshaw\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"14\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1952/10/14\",\n",
      "            \"patient_sex\": \"Female\",\n",
      "            \"blood_type\": \"A-\",\n",
      "            \"name\": \"Brian Dorsey\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"15\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1934/8/13\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"A+\",\n",
      "            \"name\": \"Olivia Gonzalez\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"16\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1932/2/9\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"O+\",\n",
      "            \"name\": \"Teresa Caldwell\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"17\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1993/11/19\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"AB+\",\n",
      "            \"name\": \"Desiree Williams MD\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"18\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1936/8/30\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"A+\",\n",
      "            \"name\": \"Sally Shaw\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"19\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1944/7/17\",\n",
      "            \"patient_sex\": \"Female\",\n",
      "            \"blood_type\": \"O-\",\n",
      "            \"name\": \"William Johnson\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"20\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1943/10/31\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"O+\",\n",
      "            \"name\": \"Steven Bennett\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"21\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1947/7/31\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"A+\",\n",
      "            \"name\": \"Haley Li\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"22\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1978/2/6\",\n",
      "            \"patient_sex\": \"Female\",\n",
      "            \"blood_type\": \"A+\",\n",
      "            \"name\": \"Angela Brown\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"23\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"n\": {\n",
      "            \"patient_dob\": \"1987/5/1\",\n",
      "            \"patient_sex\": \"Male\",\n",
      "            \"blood_type\": \"B+\",\n",
      "            \"name\": \"Beverly Miller\",\n",
      "            \"T\\r\": \"1\",\n",
      "            \"id\": \"24\"\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 定义一个工具包，包含执行Cypher查询的方法\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "import json\n",
    "\n",
    "graph= Neo4jGraph(\n",
    "    url=\"bolt://47.100.39.50:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"Fkc@1234\",\n",
    "    database=\"neo4j\")\n",
    "\n",
    "\n",
    "class MyCustomToolkit:\n",
    "    def __init__(self, graph: Neo4jGraph):\n",
    "        self._driver = graph\n",
    "\n",
    "    def execute_cypher_query(self, cypher_query:str = None, **params):\n",
    "        # 执行Cypher查询并返回结果\n",
    "        try:\n",
    "            return json.dumps(self._driver.query(cypher_query),indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        \n",
    "\n",
    "toolkit = MyCustomToolkit(graph)\n",
    "\n",
    "# 定义Cypher查询\n",
    "cypher_query = \"MATCH (n:Patient) RETURN n LIMIT 25\"\n",
    "\n",
    "# 调用函数并传入toolkit的Cypher查询\n",
    "print(toolkit.execute_cypher_query(cypher_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、定义与Neo4j交互的Agent与Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T04:38:07.931129Z",
     "iopub.status.busy": "2024-05-14T04:38:07.930568Z",
     "iopub.status.idle": "2024-05-14T04:38:07.935203Z",
     "shell.execute_reply": "2024-05-14T04:38:07.934634Z",
     "shell.execute_reply.started": "2024-05-14T04:38:07.931105Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "from langchain import PromptTemplate\n",
    "cypher_generation_template = \"\"\"\n",
    "Task:\n",
    "Generate Cypher query for a Neo4j graph database.\n",
    "\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Note:\n",
    "Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything other than\n",
    "for you to construct a Cypher statement. Do not include any text except\n",
    "the generated Cypher statement. Make sure the direction of the relationship is\n",
    "correct in your queries. Make sure you alias both entities and relationships\n",
    "properly. Do not run any queries that would add to or delete from\n",
    "the database. Make sure to alias all statements that follow as with\n",
    "statement (e.g. WITH v as visit, c.billing_amount as billing_amount)\n",
    "If you need to divide numbers, make sure to\n",
    "filter the denominator to be non zero.\n",
    "\n",
    "Examples:\n",
    "# Who is the oldest patient and how old are they?\n",
    "MATCH (p:Patient)\n",
    "RETURN p.name AS oldest_patient,\n",
    "       duration.between(date(p.dob), date()).years AS age\n",
    "ORDER BY age DESC\n",
    "LIMIT 1\n",
    "\n",
    "# Which physician has billed the least to Cigna\n",
    "MATCH (p:Payer)<-[c:COVERED_BY]-(v:Visit)-[t:TREATS]-(phy:Physician)\n",
    "WHERE p.name = 'Cigna'\n",
    "RETURN phy.name AS physician_name, SUM(c.billing_amount) AS total_billed\n",
    "ORDER BY total_billed\n",
    "LIMIT 1\n",
    "\n",
    "# Which state had the largest percent increase in Cigna visits\n",
    "# from 2022 to 2023?\n",
    "MATCH (h:Hospital)<-[:AT]-(v:Visit)-[:COVERED_BY]->(p:Payer)\n",
    "WHERE p.name = 'Cigna' AND v.admission_date >= '2022-01-01' AND\n",
    "v.admission_date < '2024-01-01'\n",
    "WITH h.state_name AS state, COUNT(v) AS visit_count,\n",
    "     SUM(CASE WHEN v.admission_date >= '2022-01-01' AND\n",
    "     v.admission_date < '2023-01-01' THEN 1 ELSE 0 END) AS count_2022,\n",
    "     SUM(CASE WHEN v.admission_date >= '2023-01-01' AND\n",
    "     v.admission_date < '2024-01-01' THEN 1 ELSE 0 END) AS count_2023\n",
    "WITH state, visit_count, count_2022, count_2023,\n",
    "     (toFloat(count_2023) - toFloat(count_2022)) / toFloat(count_2022) * 100\n",
    "     AS percent_increase\n",
    "RETURN state, percent_increase\n",
    "ORDER BY percent_increase DESC\n",
    "LIMIT 1\n",
    "\n",
    "# How many non-emergency patients in North Carolina have written reviews?\n",
    "MATCH (r:Review)<-[:WRITES]-(v:Visit)-[:AT]->(h:Hospital)\n",
    "WHERE h.state_name = 'NC' and v.admission_type <> 'Emergency'\n",
    "RETURN count(*)\n",
    "\n",
    "String category values:\n",
    "Test results are one of: 'Inconclusive', 'Normal', 'Abnormal'\n",
    "Visit statuses are one of: 'OPEN', 'DISCHARGED'\n",
    "Admission Types are one of: 'Elective', 'Emergency', 'Urgent'\n",
    "Payer names are one of: 'Cigna', 'Blue Cross', 'UnitedHealthcare', 'Medicare',\n",
    "'Aetna'\n",
    "\n",
    "A visit is considered open if its status is 'OPEN' and the discharge date is\n",
    "missing.\n",
    "Use abbreviations when\n",
    "filtering on hospital states (e.g. \"Texas\" is \"TX\",\n",
    "\"Colorado\" is \"CO\", \"North Carolina\" is \"NC\",\n",
    "\"Florida\" is \"FL\", \"Georgia\" is \"GA\", etc.)\n",
    "\n",
    "Make sure to use IS NULL or IS NOT NULL when analyzing missing properties.\n",
    "Never return embedding properties in your queries. You must never include the\n",
    "statement \"GROUP BY\" in your query. Make sure to alias all statements that\n",
    "follow as with statement (e.g. WITH v as visit, c.billing_amount as\n",
    "billing_amount)\n",
    "If you need to divide numbers, make sure to filter the denominator to be non\n",
    "zero.\n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "cypher_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=cypher_generation_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T04:38:23.609387Z",
     "iopub.status.busy": "2024-05-14T04:38:23.609023Z",
     "iopub.status.idle": "2024-05-14T04:38:23.613665Z",
     "shell.execute_reply": "2024-05-14T04:38:23.613048Z",
     "shell.execute_reply.started": "2024-05-14T04:38:23.609366Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "\n",
    "qa_generation_template_en = \"\"\"You are an assistant that takes the results\n",
    "from a Neo4j Cypher query and forms a human-readable response. The\n",
    "query results section contains the results of a Cypher query that was\n",
    "generated based on a user's natural language question. The provided\n",
    "information is authoritative, you must never doubt it or try to use\n",
    "your internal knowledge to correct it. Make the answer sound like a\n",
    "response to the question.\n",
    "\n",
    "Query Results:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "If the provided information is empty, say you don't know the answer.\n",
    "Empty information looks like this: [{context} , the provided information is empty ]\n",
    "\n",
    "If the information is not empty, you must provide an answer using the\n",
    "results. If the question involves a time duration, assume the query\n",
    "results are in units of days unless otherwise specified.\n",
    "\n",
    "When names are provided in the query results, such as hospital names,\n",
    "beware  of any names that have commas or other punctuation in them.\n",
    "For instance, 'Jones, Brown and Murray' is a single hospital name,\n",
    "not multiple hospitals. Make sure you return any list of names in\n",
    "a way that isn't ambiguous and allows someone to tell what the full\n",
    "names are.\n",
    "\n",
    "Never say you don't have the right information if there is data in\n",
    "the query results. Always use the data in the query results.\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "qa_generation_template = \"\"\"You are an assistant that takes the results\n",
    "from a Neo4j Cypher query and forms a human-readable response. The\n",
    "query results section contains the results of a Cypher query that was\n",
    "generated based on a user's natural language question. The provided\n",
    "information is authoritative, you must never doubt it or try to use\n",
    "your internal knowledge to correct it. Make the answer sound like a\n",
    "response to the question.\n",
    "\n",
    "Query Results:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "If the provided information is empty, say you don't know the answer.\n",
    "Empty information looks like this: []\n",
    "\n",
    "If the information is not empty, you must provide an answer using the\n",
    "results. If the question involves a time duration, assume the query\n",
    "results are in units of days unless otherwise specified.\n",
    "\n",
    "When names are provided in the query results, such as hospital names,\n",
    "beware  of any names that have commas or other punctuation in them.\n",
    "For instance, 'Jones, Brown and Murray' is a single hospital name,\n",
    "not multiple hospitals. Make sure you return any list of names in\n",
    "a way that isn't ambiguous and allows someone to tell what the full\n",
    "names are.\n",
    "\n",
    "Never say you don't have the right information if there is data in\n",
    "the query results. Always use the data in the query results.\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "qa_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=qa_generation_template_en\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-14T04:38:39.234489Z",
     "iopub.status.busy": "2024-05-14T04:38:39.234166Z",
     "iopub.status.idle": "2024-05-14T04:38:57.676385Z",
     "shell.execute_reply": "2024-05-14T04:38:57.675859Z",
     "shell.execute_reply.started": "2024-05-14T04:38:39.234467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "graph= Neo4jGraph(\n",
    "    url=\"bolt://47.100.39.50:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"Fkc@1234\",\n",
    "    database=\"neo4j\")\n",
    "\n",
    "customLLM = ChatGLM3()\n",
    "customLLM.setClient(client=chatbot)\n",
    "customLLM.invoke(\"你是谁？\")\n",
    "\n",
    "from langchain.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "hospital_cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=customLLM,\n",
    "    qa_llm=customLLM,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    qa_prompt=qa_generation_prompt,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    validate_cypher=True,\n",
    "    top_k=100,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-14T04:55:57.705850Z",
     "iopub.status.busy": "2024-05-14T04:55:57.705310Z",
     "iopub.status.idle": "2024-05-14T05:00:02.343156Z",
     "shell.execute_reply": "2024-05-14T05:00:02.342538Z",
     "shell.execute_reply.started": "2024-05-14T04:55:57.705811Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Patient) WHERE p.name = 'Christina Williams' RETURN p.id AS patient_id,\n",
      "  p.blood_type AS blood_type\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'patient_id': '12', 'blood_type': 'O-'}, {'patient_id': '8815', 'blood_type': 'AB+'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "服务调用花费了 244.63360571861267 秒\n"
     ]
    }
   ],
   "source": [
    "#question = \"\"\"请查找患者名字是'Patty Norman' 的入院时间和出院时间\"\"\"\n",
    "import time\n",
    "start_time = time.time()\n",
    "question = \"\"\"Please look for the ID and blood type of the Patient named \"Christina Williams\", I think the field name of blood type will be like patient blood type. \"\"\"\n",
    "response = hospital_cypher_chain.invoke(question)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"服务调用花费了 {elapsed_time} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Intel量化模式，验证使用效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:20:58.633068Z",
     "iopub.status.busy": "2024-05-08T15:20:58.632643Z",
     "iopub.status.idle": "2024-05-08T15:25:56.936258Z",
     "shell.execute_reply": "2024-05-08T15:25:56.935770Z",
     "shell.execute_reply.started": "2024-05-08T15:20:58.633038Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 23:20:59,903 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: ./bge-base-zh-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create retrieval plugin instance...\n",
      "plugin parameters:  {'embedding_model': './bge-base-zh-v1.5', 'input_path': './sample.jsonl'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 23:21:01,417 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
      "2024-05-08 23:21:01,420 - root - INFO - The parsing for the uploaded files is finished.\n",
      "2024-05-08 23:21:01,420 - root - INFO - The format of parsed documents is transferred.\n",
      "2024-05-08 23:21:01,435 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3043b0806f8c4ee58c831aa5fc7b9e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 23:21:01,915 - root - INFO - The retriever is successfully built.\n",
      "2024-05-08 23:21:01,947 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting eos_token is not supported, use the default one.\n",
      "2024-05-08 23:21:01,947 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting pad_token is not supported, use the default one.\n",
      "2024-05-08 23:21:01,948 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting unk_token is not supported, use the default one.\n",
      "2024-05-08 23:21:02 [INFO] Applying Weight Only Quantization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ./chatglm3-6b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e0255093284b57a6042336fe989338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 23:21:36 [INFO] Start auto tuning.\n",
      "2024-05-08 23:21:36 [INFO] Quantize model without tuning!\n",
      "2024-05-08 23:21:36 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2024-05-08 23:21:36 [INFO] Adaptor has 5 recipes.\n",
      "2024-05-08 23:21:36 [INFO] 0 recipes specified by user.\n",
      "2024-05-08 23:21:36 [INFO] 3 recipes require future tuning.\n",
      "2024-05-08 23:21:36 [INFO] *** Initialize auto tuning\n",
      "2024-05-08 23:21:36 [INFO] {\n",
      "2024-05-08 23:21:36 [INFO]     'PostTrainingQuantConfig': {\n",
      "2024-05-08 23:21:36 [INFO]         'AccuracyCriterion': {\n",
      "2024-05-08 23:21:36 [INFO]             'criterion': 'relative',\n",
      "2024-05-08 23:21:36 [INFO]             'higher_is_better': True,\n",
      "2024-05-08 23:21:36 [INFO]             'tolerable_loss': 0.01,\n",
      "2024-05-08 23:21:36 [INFO]             'absolute': None,\n",
      "2024-05-08 23:21:36 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7f497402ca90>>,\n",
      "2024-05-08 23:21:36 [INFO]             'relative': 0.01\n",
      "2024-05-08 23:21:36 [INFO]         },\n",
      "2024-05-08 23:21:36 [INFO]         'approach': 'post_training_weight_only',\n",
      "2024-05-08 23:21:36 [INFO]         'backend': 'default',\n",
      "2024-05-08 23:21:36 [INFO]         'calibration_sampling_size': [\n",
      "2024-05-08 23:21:36 [INFO]             100\n",
      "2024-05-08 23:21:36 [INFO]         ],\n",
      "2024-05-08 23:21:36 [INFO]         'device': 'cpu',\n",
      "2024-05-08 23:21:36 [INFO]         'diagnosis': False,\n",
      "2024-05-08 23:21:36 [INFO]         'domain': 'auto',\n",
      "2024-05-08 23:21:36 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2024-05-08 23:21:36 [INFO]         'excluded_precisions': [\n",
      "2024-05-08 23:21:36 [INFO]         ],\n",
      "2024-05-08 23:21:36 [INFO]         'framework': 'pytorch_fx',\n",
      "2024-05-08 23:21:36 [INFO]         'inputs': [\n",
      "2024-05-08 23:21:36 [INFO]         ],\n",
      "2024-05-08 23:21:36 [INFO]         'model_name': '',\n",
      "2024-05-08 23:21:36 [INFO]         'ni_workload_name': 'quantization',\n",
      "2024-05-08 23:21:36 [INFO]         'op_name_dict': {\n",
      "2024-05-08 23:21:36 [INFO]             '.*lm_head': {\n",
      "2024-05-08 23:21:36 [INFO]                 'weight': {\n",
      "2024-05-08 23:21:36 [INFO]                     'dtype': [\n",
      "2024-05-08 23:21:36 [INFO]                         'fp32'\n",
      "2024-05-08 23:21:36 [INFO]                     ]\n",
      "2024-05-08 23:21:36 [INFO]                 }\n",
      "2024-05-08 23:21:36 [INFO]             },\n",
      "2024-05-08 23:21:36 [INFO]             '.*output_layer': {\n",
      "2024-05-08 23:21:36 [INFO]                 'weight': {\n",
      "2024-05-08 23:21:36 [INFO]                     'dtype': [\n",
      "2024-05-08 23:21:36 [INFO]                         'fp32'\n",
      "2024-05-08 23:21:36 [INFO]                     ]\n",
      "2024-05-08 23:21:36 [INFO]                 }\n",
      "2024-05-08 23:21:36 [INFO]             },\n",
      "2024-05-08 23:21:36 [INFO]             '.*embed_out': {\n",
      "2024-05-08 23:21:36 [INFO]                 'weight': {\n",
      "2024-05-08 23:21:36 [INFO]                     'dtype': [\n",
      "2024-05-08 23:21:36 [INFO]                         'fp32'\n",
      "2024-05-08 23:21:36 [INFO]                     ]\n",
      "2024-05-08 23:21:36 [INFO]                 }\n",
      "2024-05-08 23:21:36 [INFO]             }\n",
      "2024-05-08 23:21:36 [INFO]         },\n",
      "2024-05-08 23:21:36 [INFO]         'op_type_dict': {\n",
      "2024-05-08 23:21:36 [INFO]             '.*': {\n",
      "2024-05-08 23:21:36 [INFO]                 'weight': {\n",
      "2024-05-08 23:21:36 [INFO]                     'bits': [\n",
      "2024-05-08 23:21:36 [INFO]                         4\n",
      "2024-05-08 23:21:36 [INFO]                     ],\n",
      "2024-05-08 23:21:36 [INFO]                     'dtype': [\n",
      "2024-05-08 23:21:36 [INFO]                         'int4'\n",
      "2024-05-08 23:21:36 [INFO]                     ],\n",
      "2024-05-08 23:21:36 [INFO]                     'group_size': [\n",
      "2024-05-08 23:21:36 [INFO]                         32\n",
      "2024-05-08 23:21:36 [INFO]                     ],\n",
      "2024-05-08 23:21:36 [INFO]                     'scheme': [\n",
      "2024-05-08 23:21:36 [INFO]                         'sym'\n",
      "2024-05-08 23:21:36 [INFO]                     ],\n",
      "2024-05-08 23:21:36 [INFO]                     'algorithm': [\n",
      "2024-05-08 23:21:36 [INFO]                         'RTN'\n",
      "2024-05-08 23:21:36 [INFO]                     ]\n",
      "2024-05-08 23:21:36 [INFO]                 }\n",
      "2024-05-08 23:21:36 [INFO]             }\n",
      "2024-05-08 23:21:36 [INFO]         },\n",
      "2024-05-08 23:21:36 [INFO]         'outputs': [\n",
      "2024-05-08 23:21:36 [INFO]         ],\n",
      "2024-05-08 23:21:36 [INFO]         'quant_format': 'default',\n",
      "2024-05-08 23:21:36 [INFO]         'quant_level': 'auto',\n",
      "2024-05-08 23:21:36 [INFO]         'recipes': {\n",
      "2024-05-08 23:21:36 [INFO]             'smooth_quant': False,\n",
      "2024-05-08 23:21:36 [INFO]             'smooth_quant_args': {\n",
      "2024-05-08 23:21:36 [INFO]             },\n",
      "2024-05-08 23:21:36 [INFO]             'layer_wise_quant': False,\n",
      "2024-05-08 23:21:36 [INFO]             'layer_wise_quant_args': {\n",
      "2024-05-08 23:21:36 [INFO]             },\n",
      "2024-05-08 23:21:36 [INFO]             'fast_bias_correction': False,\n",
      "2024-05-08 23:21:36 [INFO]             'weight_correction': False,\n",
      "2024-05-08 23:21:36 [INFO]             'gemm_to_matmul': True,\n",
      "2024-05-08 23:21:36 [INFO]             'graph_optimization_level': None,\n",
      "2024-05-08 23:21:36 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2024-05-08 23:21:36 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2024-05-08 23:21:36 [INFO]             'pre_post_process_quantization': True,\n",
      "2024-05-08 23:21:36 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2024-05-08 23:21:36 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2024-05-08 23:21:36 [INFO]             ],\n",
      "2024-05-08 23:21:36 [INFO]             'dedicated_qdq_pair': False,\n",
      "2024-05-08 23:21:36 [INFO]             'rtn_args': {\n",
      "2024-05-08 23:21:36 [INFO]                 'enable_full_range': True,\n",
      "2024-05-08 23:21:36 [INFO]                 'enable_mse_search': False\n",
      "2024-05-08 23:21:36 [INFO]             },\n",
      "2024-05-08 23:21:36 [INFO]             'awq_args': {\n",
      "2024-05-08 23:21:36 [INFO]             },\n",
      "2024-05-08 23:21:36 [INFO]             'gptq_args': {\n",
      "2024-05-08 23:21:36 [INFO]             },\n",
      "2024-05-08 23:21:36 [INFO]             'teq_args': {\n",
      "2024-05-08 23:21:36 [INFO]             },\n",
      "2024-05-08 23:21:36 [INFO]             'autoround_args': {\n",
      "2024-05-08 23:21:36 [INFO]             }\n",
      "2024-05-08 23:21:36 [INFO]         },\n",
      "2024-05-08 23:21:36 [INFO]         'reduce_range': None,\n",
      "2024-05-08 23:21:36 [INFO]         'TuningCriterion': {\n",
      "2024-05-08 23:21:36 [INFO]             'max_trials': 100,\n",
      "2024-05-08 23:21:36 [INFO]             'objective': [\n",
      "2024-05-08 23:21:36 [INFO]                 'performance'\n",
      "2024-05-08 23:21:36 [INFO]             ],\n",
      "2024-05-08 23:21:36 [INFO]             'strategy': 'basic',\n",
      "2024-05-08 23:21:36 [INFO]             'strategy_kwargs': None,\n",
      "2024-05-08 23:21:36 [INFO]             'timeout': 0\n",
      "2024-05-08 23:21:36 [INFO]         },\n",
      "2024-05-08 23:21:36 [INFO]         'use_bf16': True\n",
      "2024-05-08 23:21:36 [INFO]     }\n",
      "2024-05-08 23:21:36 [INFO] }\n",
      "2024-05-08 23:21:36 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2024-05-08 23:21:36 [INFO] Pass query framework capability elapsed time: 2.63 ms\n",
      "2024-05-08 23:21:36 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2024-05-08 23:21:36 [INFO] Quantize the model with default config.\n",
      "2024-05-08 23:21:36 [INFO] All algorithms to do: {'RTN'}\n",
      "2024-05-08 23:21:36 [INFO] quantizing with the round-to-nearest algorithm\n",
      "2024-05-08 23:21:47 [INFO] |******Mixed Precision Statistics******|\n",
      "2024-05-08 23:21:47 [INFO] +---------+-------+-----------+--------+\n",
      "2024-05-08 23:21:47 [INFO] | Op Type | Total |  A32W4G32 |  FP32  |\n",
      "2024-05-08 23:21:47 [INFO] +---------+-------+-----------+--------+\n",
      "2024-05-08 23:21:47 [INFO] |  Linear |  113  |    112    |   1    |\n",
      "2024-05-08 23:21:47 [INFO] +---------+-------+-----------+--------+\n",
      "2024-05-08 23:21:47 [INFO] Pass quantize model elapsed time: 10631.59 ms\n",
      "2024-05-08 23:21:47 [INFO] Save tuning history to /mnt/workspace/Intel_RagNeo4j/nc_workspace/2024-05-08_22-33-18/./history.snapshot.\n",
      "2024-05-08 23:21:47 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2024-05-08 23:21:47 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2024-05-08 23:21:47 [INFO] Save deploy yaml to /mnt/workspace/Intel_RagNeo4j/nc_workspace/2024-05-08_22-33-18/deploy.yaml\n",
      "2024-05-08 23:25:56 [INFO] WeightOnlyQuant done.\n",
      "2024-05-08 23:25:56,934 - root - INFO - Optimized Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "from intel_extension_for_transformers.transformers import RtnConfig\n",
    "plugins.retrieval.enable=True\n",
    "plugins.retrieval.args['embedding_model'] = \"./bge-base-zh-v1.5\"\n",
    "plugins.retrieval.args[\"input_path\"]=\"./sample.jsonl\"\n",
    "config_int8 = PipelineConfig(model_name_or_path='./chatglm3-6b',\n",
    " plugins=plugins,\n",
    " optimization_config=RtnConfig(compute_dtype=\"int8\",\n",
    "weight_dtype=\"int4_fullrange\"))\n",
    "chatbot_int8 = build_chatbot(config_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T15:34:01.008347Z",
     "iopub.status.busy": "2024-05-08T15:34:01.008027Z",
     "iopub.status.idle": "2024-05-08T15:41:21.762994Z",
     "shell.execute_reply": "2024-05-08T15:41:21.761853Z",
     "shell.execute_reply.started": "2024-05-08T15:34:01.008328Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/torch/amp/autocast_mode.py:267: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c97c8d12bc4fa5ae1e44fe8d4ac8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 23:39:24,953 - root - INFO - Chat with QA Agent.\n",
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/torch/amp/autocast_mode.py:267: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mPlease find the ID and blood type of the Patient named \"Christina Williams\" in the table.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Generated Cypher Statement is not valid\n{code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'Please': expected\n  \"ALTER\"\n  \"CALL\"\n  \"CREATE\"\n  \"DEALLOCATE\"\n  \"DELETE\"\n  \"DENY\"\n  \"DETACH\"\n  \"DROP\"\n  \"DRYRUN\"\n  \"ENABLE\"\n  \"FOREACH\"\n  \"GRANT\"\n  \"LOAD\"\n  \"MATCH\"\n  \"MERGE\"\n  \"OPTIONAL\"\n  \"REALLOCATE\"\n  \"REMOVE\"\n  \"RENAME\"\n  \"RETURN\"\n  \"REVOKE\"\n  \"SET\"\n  \"SHOW\"\n  \"START\"\n  \"STOP\"\n  \"TERMINATE\"\n  \"UNWIND\"\n  \"USE\"\n  \"USING\"\n  \"WITH\" (line 1, column 1 (offset: 0))\n\"Please find the ID and blood type of the Patient named \"Christina Williams\" in the table.\"\n ^}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCypherSyntaxError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/langchain_community/graphs/neo4j_graph.py:232\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[0;34m(self, query, params)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/neo4j/_sync/work/session.py:313\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpersonated_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbookmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_min_severity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotifications_disabled_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_result\n",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/neo4j/_sync/work/result.py:181\u001b[0m, in \u001b[0;36mResult._run\u001b[0;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_categories)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/neo4j/_sync/work/result.py:301\u001b[0m, in \u001b[0;36mResult._attach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/neo4j/_sync/io/_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/neo4j/_sync/io/_bolt.py:850\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    848\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[1;32m    849\u001b[0m )\n\u001b[0;32m--> 850\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/neo4j/_sync/io/_bolt5.py:369\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/neo4j/_sync/io/_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
      "\u001b[0;31mCypherSyntaxError\u001b[0m: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'Please': expected\n  \"ALTER\"\n  \"CALL\"\n  \"CREATE\"\n  \"DEALLOCATE\"\n  \"DELETE\"\n  \"DENY\"\n  \"DETACH\"\n  \"DROP\"\n  \"DRYRUN\"\n  \"ENABLE\"\n  \"FOREACH\"\n  \"GRANT\"\n  \"LOAD\"\n  \"MATCH\"\n  \"MERGE\"\n  \"OPTIONAL\"\n  \"REALLOCATE\"\n  \"REMOVE\"\n  \"RENAME\"\n  \"RETURN\"\n  \"REVOKE\"\n  \"SET\"\n  \"SHOW\"\n  \"START\"\n  \"STOP\"\n  \"TERMINATE\"\n  \"UNWIND\"\n  \"USE\"\n  \"USING\"\n  \"WITH\" (line 1, column 1 (offset: 0))\n\"Please find the ID and blood type of the Patient named \"Christina Williams\" in the table.\"\n ^}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#question = \"\"\"请查找患者名字是'Patty Norman' 的入院时间和出院时间\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mPlease look for the ID and blood type of the Patient named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChristina Williams\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, I think the field name of blood type will be like patient blood type. \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mhospital_cypher_chain_int8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/langchain/chains/graph_qa/cypher.py:267\u001b[0m, in \u001b[0;36mGraphCypherQAChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Retrieve and limit the number of results\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Generated Cypher be null if query corrector identifies invalid schema\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generated_cypher:\n\u001b[0;32m--> 267\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_cypher\u001b[49m\u001b[43m)\u001b[49m[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     context \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/envs/itrex/lib/python3.10/site-packages/langchain_community/graphs/neo4j_graph.py:238\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[0;34m(self, query, params)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_data\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CypherSyntaxError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Cypher Statement is not valid\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Generated Cypher Statement is not valid\n{code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'Please': expected\n  \"ALTER\"\n  \"CALL\"\n  \"CREATE\"\n  \"DEALLOCATE\"\n  \"DELETE\"\n  \"DENY\"\n  \"DETACH\"\n  \"DROP\"\n  \"DRYRUN\"\n  \"ENABLE\"\n  \"FOREACH\"\n  \"GRANT\"\n  \"LOAD\"\n  \"MATCH\"\n  \"MERGE\"\n  \"OPTIONAL\"\n  \"REALLOCATE\"\n  \"REMOVE\"\n  \"RENAME\"\n  \"RETURN\"\n  \"REVOKE\"\n  \"SET\"\n  \"SHOW\"\n  \"START\"\n  \"STOP\"\n  \"TERMINATE\"\n  \"UNWIND\"\n  \"USE\"\n  \"USING\"\n  \"WITH\" (line 1, column 1 (offset: 0))\n\"Please find the ID and blood type of the Patient named \"Christina Williams\" in the table.\"\n ^}"
     ]
    }
   ],
   "source": [
    "customLLM_int8 = ChatGLM3()\n",
    "customLLM_int8.setClient(client=chatbot_int8)\n",
    "# customLLM_int8.invoke(\"你是谁？\")\n",
    "\n",
    "from langchain.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "hospital_cypher_chain_int8 = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=customLLM_int8,\n",
    "    qa_llm=customLLM_int8,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    qa_prompt=qa_generation_prompt,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    validate_cypher=True,\n",
    "    top_k=100,    \n",
    ")\n",
    "\n",
    "#question = \"\"\"请查找患者名字是'Patty Norman' 的入院时间和出院时间\"\"\"\n",
    "question = \"\"\"Please look for the ID and blood type of the Patient named \"Christina Williams\", I think the field name of blood type will be like patient blood type. \"\"\"\n",
    "response = hospital_cypher_chain_int8.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
